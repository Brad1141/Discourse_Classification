{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (C:\\Users\\Brad\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n",
      "100%|██████████| 3/3 [00:00<00:00, 95.53it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_datasets  = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileToArray(file_path):\n",
    "    # turn text into array of words\n",
    "    text_ds = tf.data.TextLineDataset(file_path).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "    text_ds = text_ds.enumerate()\n",
    "    docTxt = []\n",
    "    for i in text_ds.as_numpy_iterator():\n",
    "        line = i[1].decode().split()\n",
    "        for l in line:\n",
    "            docTxt.append(l)\n",
    "    \n",
    "    docTxt = ' '.join(docTxt)\n",
    "    return docTxt\n",
    "\n",
    "def calc_word_indices(full_text, discourse_start, discourse_end):\n",
    "    start_index = len(full_text[:discourse_start].split())\n",
    "    token_len = len(full_text[discourse_start:discourse_end].split())\n",
    "    output = list(range(start_index, start_index + token_len))\n",
    "    if output[-1] >= len(full_text.split()):\n",
    "        output = list(range(start_index, start_index + token_len-1))\n",
    "    return output\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def stringToArray(mystr):\n",
    "    wordList = re.sub(\"[^\\w]\", \" \",  mystr).split()\n",
    "    return wordList\n",
    "\n",
    "def getIntArray(strArray):\n",
    "    intArray = []\n",
    "    for s in strArray:\n",
    "        intArray.append(int(s))\n",
    "    return intArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>313.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>Some certain areas in the United States ban ph...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>402.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>When people have phones, they know about certa...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>759.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>Driving is one of the way how to get around. P...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "2  423A1CA112E2  1.622628e+12            313.0          401.0   \n",
       "3  423A1CA112E2  1.622628e+12            402.0          758.0   \n",
       "4  423A1CA112E2  1.622628e+12            759.0          886.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "1  They are some really bad consequences when stu...       Position   \n",
       "2  Some certain areas in the United States ban ph...       Evidence   \n",
       "3  When people have phones, they know about certa...       Evidence   \n",
       "4  Driving is one of the way how to get around. P...          Claim   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
       "1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n",
       "2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n",
       "3         Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n",
       "4            Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'80 81 82 83 84 85 86 87 88 89 90 91 92'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"predictionstring\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]\n"
     ]
    }
   ],
   "source": [
    "sampleTxt = fileToArray(\"train/\" + str(train[\"id\"][0]) + \".txt\")\n",
    "ftest = calc_word_indices(sampleTxt, int(train[\"discourse_start\"][0]), int(train[\"discourse_end\"][0]))\n",
    "print(ftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>class</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, class, predictionstring]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#make a dummy submission\n",
    "data = {'id': [], 'class': [], 'predictionstring': []}\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('test', [], ['0FB0700DAF44.txt', '18409261F5C2.txt', 'D46BCB48440A.txt', 'D72CB1C11673.txt', 'DF920E0A7337.txt'])\n"
     ]
    }
   ],
   "source": [
    "for t in os.walk('test'):\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n",
       "       'Counterclaim', 'Rebuttal'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"discourse_type\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dictionary to convert discourse types to integers\n",
    "tokenDict = {\n",
    "    \"Lead\" : 0,\n",
    "    \"Position\" : 1,\n",
    "    \"Evidence\" : 2,\n",
    "    \"Claim\" : 3,\n",
    "    \"Concluding Statement\" : 4,\n",
    "    \"Counterclaim\" : 5,\n",
    "    \"Rebuttal\": 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Modern', 'humans', 'today', 'are', 'always', 'on', 'their', 'phone.', 'They', 'are', 'always', 'on', 'their', 'phone', 'more', 'than', '5', 'hours', 'a', 'day', 'no', 'stop', '.All', 'they', 'do', 'is', 'text', 'back', 'and', 'forward', 'and', 'just', 'have', 'group', 'Chats', 'on', 'social', 'media.', 'They', 'even', 'do', 'it', 'while', 'driving.', 'They', 'are', 'some', 'really', 'bad', 'consequences', 'when', 'stuff', 'happens', 'when', 'it', 'comes', 'to', 'a', 'phone.', 'Some', 'certain', 'areas', 'in', 'the', 'United', 'States', 'ban', 'phones', 'from', 'class', 'rooms', 'just', 'because', 'of', 'it.', 'When', 'people', 'have', 'phones,', 'they', 'know', 'about', 'certain', 'apps', 'that', 'they', 'have', '.Apps', 'like', 'Facebook', 'Twitter', 'Instagram', 'and', 'Snapchat.', 'So', 'like', 'if', 'a', 'friend', 'moves', 'away', 'and', 'you', 'want', 'to', 'be', 'in', 'contact', 'you', 'can', 'still', 'be', 'in', 'contact', 'by', 'posting', 'videos', 'or', 'text', 'messages.', 'People', 'always', 'have', 'different', 'ways', 'how', 'to', 'communicate', 'with', 'a', 'phone.', 'Phones', 'have', 'changed', 'due', 'to', 'our', 'generation.', 'Driving', 'is', 'one', 'of', 'the', 'way', 'how', 'to', 'get', 'around.', 'People', 'always', 'be', 'on', 'their', 'phones', 'while', 'doing', 'it.', 'Which', 'can', 'cause', 'serious', 'Problems.', \"That's\", 'why', \"there's\", 'a', 'thing', \"that's\", 'called', 'no', 'texting', 'while', 'driving.', \"That's\", 'a', 'really', 'important', 'thing', 'to', 'remember.', 'Some', 'people', 'still', 'do', 'it', 'because', 'they', 'think', \"It's\", 'stupid.', 'No', 'matter', 'what', 'they', 'do', 'they', 'still', 'have', 'to', 'obey', 'it', 'because', \"that's\", 'the', 'only', 'way', 'how', 'did', 'he', 'save.', 'Sometimes', 'on', 'the', 'news', 'there', 'is', 'either', 'an', 'accident', 'or', 'a', 'suicide.', 'It', 'might', 'involve', 'someone', 'not', 'looking', 'where', \"they're\", 'going', 'or', 'tweet', 'that', 'someone', 'sent.', 'It', 'either', 'injury', 'or', 'death.', 'If', 'a', 'mysterious', 'number', 'says', \"I'm\", 'going', 'to', 'kill', 'you', 'and', 'they', 'know', 'where', 'you', 'live', 'but', 'you', \"don't\", 'know', 'the', \"person's\", 'contact', ',It', 'makes', 'you', 'puzzled', 'and', 'make', 'you', 'start', 'to', 'freak', 'out.', 'Which', 'can', 'end', 'up', 'really', 'badly.', 'Phones', 'are', 'fine', 'to', 'use', 'and', \"it's\", 'also', 'the', 'best', 'way', 'to', 'come', 'over', 'help.', 'If', 'you', 'go', 'through', 'a', 'problem', 'and', 'you', \"can't\", 'find', 'help', 'you', ',always', 'have', 'a', 'phone', 'there', 'with', 'you.', 'Even', 'though', 'phones', 'are', 'used', 'almost', 'every', 'day', 'as', 'long', 'as', \"you're\", 'safe', 'it', 'would', 'come', 'into', 'use', 'if', 'you', 'get', 'into', 'trouble.', 'Make', 'sure', 'you', 'do', 'not', 'be', 'like', 'this', 'phone', 'while', \"you're\", 'in', 'the', 'middle', 'of', 'driving.', 'The', 'news', 'always', 'updated', 'when', 'people', 'do', 'something', 'stupid', 'around', 'that', 'involves', 'their', 'phones.', 'The', 'safest', 'way', 'is', 'the', 'best', 'way', 'to', 'stay', 'safe.']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "df_file = train[train.id.str.contains(str(train[\"id\"][0]),case=False)]\n",
    "labels = []\n",
    "num_labels = []\n",
    "fullText = []\n",
    "for i in range(len(df_file.index)):\n",
    "    df_string = df_file[\"predictionstring\"][i]\n",
    "    stringArray = df_string.split()\n",
    "    df_textString = df_file[\"discourse_text\"][i].split()\n",
    "    for df in df_textString:\n",
    "        # try not appending word if it is a '.' or a ','\n",
    "        fullText.append(df)\n",
    "    for s in stringArray:\n",
    "        labels.append(df_file[\"discourse_type\"][i])\n",
    "        num_labels.append(tokenDict[str(df_file[\"discourse_type\"][i])])\n",
    "        \n",
    "\n",
    "print(fullText)\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378\n",
      "452\n",
      "[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 3, 3, 3, 3, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 4, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 4, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, -100]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(fullText, is_split_into_words=True)\n",
    "word_ids = inputs.word_ids()\n",
    "print(len(num_labels))\n",
    "print(len(word_ids))\n",
    "print(align_labels_with_tokens(num_labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8508840b1601b217\n",
      "Reusing dataset csv (C:\\Users\\Brad\\.cache\\huggingface\\datasets\\csv\\default-8508840b1601b217\\0.0.0\\6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n",
      "100%|██████████| 1/1 [00:00<00:00, 143.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# tokenize kaggle dataset through mapping\n",
    "kaggleTrain = load_dataset('csv', data_files=\"trainHugging1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    all_labels = examples[\"ner_tags\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        try: \n",
    "            word_ids = tokenized_inputs.word_ids(i)\n",
    "            new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "        except:\n",
    "            new_labels.append([-100])\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>[Modern, humans, today, are, always, on, their...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A8445CABFECE</td>\n",
       "      <td>[Drivers, should, not, be, able, to, use, phon...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6B4F7A0165B9</td>\n",
       "      <td>[The, ability, to, stay, connected, to, people...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>[People, are, debating, whether, if, drivers, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50B3435E475B</td>\n",
       "      <td>[Over, half, of, drivers, in, today's, society...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                             tokens  \\\n",
       "0  423A1CA112E2  [Modern, humans, today, are, always, on, their...   \n",
       "1  A8445CABFECE  [Drivers, should, not, be, able, to, use, phon...   \n",
       "2  6B4F7A0165B9  [The, ability, to, stay, connected, to, people...   \n",
       "3  E05C7F5C1156  [People, are, debating, whether, if, drivers, ...   \n",
       "4  50B3435E475B  [Over, half, of, drivers, in, today's, society...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run this after trainHugging4\n",
    "kaggleTrain = pd.read_csv(\"trainHugging4.csv\")\n",
    "kaggleTrain['tokens'] = kaggleTrain['tokens'].apply(lambda a: a.split())\n",
    "kaggleTrain['ner_tags'] = kaggleTrain['ner_tags'].apply(lambda a: a.split())\n",
    "kaggleTrain = kaggleTrain.drop(['Unnamed: 0'], axis = 1)\n",
    "kaggleTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in kaggleTrain.iterrows():\n",
    "    numTags = []\n",
    "    for r in row[\"ner_tags\"]:\n",
    "        numTags.append(int(r))\n",
    "    \n",
    "    kaggleTrain.at[i, \"ner_tags\"] = numTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kaggleTrain[\"ner_tags\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggleData = Dataset.from_pandas(kaggleTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(kaggleData[\"tokens\"][1], is_split_into_words=True)\n",
    "inputs.tokens()\n",
    "word_ids = inputs.word_ids()\n",
    "taggs = getIntArray(kaggleData[\"ner_tags\"][1])\n",
    "print(len(kaggleData[\"tokens\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(align_labels_with_tokens(kaggleData[\"ner_tags\"][0], word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:48<00:00,  3.04s/ba]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = kaggleData.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'id', 'input_ids', 'labels', 'ner_tags', 'token_type_ids', 'tokens'],\n",
       "    num_rows: 15594\n",
       "})"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #errors\n",
    "# C3811E7F1750\n",
    "# 9B7494278BAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>id</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>labels</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>[101, 4825, 3612, 2052, 1132, 1579, 1113, 1147...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[Modern, humans, today, are, always, on, their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>A8445CABFECE</td>\n",
       "      <td>[101, 23004, 1431, 1136, 1129, 1682, 1106, 132...</td>\n",
       "      <td>[-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[Drivers, should, not, be, able, to, use, phon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>6B4F7A0165B9</td>\n",
       "      <td>[101, 1109, 2912, 1106, 2215, 3387, 1106, 1234...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[The, ability, to, stay, connected, to, people...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>[101, 2563, 1132, 27066, 2480, 1191, 7016, 143...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[People, are, debating, whether, if, drivers, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>50B3435E475B</td>\n",
       "      <td>[101, 3278, 1544, 1104, 7016, 1107, 2052, 112,...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[Over, half, of, drivers, in, today's, society...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      attention_mask            id  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  423A1CA112E2   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  A8445CABFECE   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  6B4F7A0165B9   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  E05C7F5C1156   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  50B3435E475B   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 4825, 3612, 2052, 1132, 1579, 1113, 1147...   \n",
       "1  [101, 23004, 1431, 1136, 1129, 1682, 1106, 132...   \n",
       "2  [101, 1109, 2912, 1106, 2215, 3387, 1106, 1234...   \n",
       "3  [101, 2563, 1132, 27066, 2480, 1191, 7016, 143...   \n",
       "4  [101, 3278, 1544, 1104, 7016, 1107, 2052, 112,...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [-100, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, ...   \n",
       "2  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                            ner_tags  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                      token_type_ids  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [Modern, humans, today, are, always, on, their...  \n",
       "1  [Drivers, should, not, be, able, to, use, phon...  \n",
       "2  [The, ability, to, stay, connected, to, people...  \n",
       "3  [People, are, debating, whether, if, drivers, ...  \n",
       "4  [Over, half, of, drivers, in, today's, society...  "
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_token = tokenized_datasets.to_pandas()\n",
    "df_token.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tokenError = pd.DataFrame(columns=[\"id\", \"labels\", \"ner_tags\", \"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_token.iterrows():\n",
    "    if len(row[\"labels\"]) < 5:\n",
    "        df_tokenError.loc[len(df_tokenError)] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C3811E7F1750</td>\n",
       "      <td>[-100]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[BOOM!!, You're, on, I-75, on, the, ground, bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9B7494278BAE</td>\n",
       "      <td>[-100]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[Even, though, majority, of, humans, own, and,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>93B9C33FF16D</td>\n",
       "      <td>[-100]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[Their, are, so, many, things, you, can, do, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABE35EAFEC00</td>\n",
       "      <td>[-100]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[more, people, die, in, car, related, accidnts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>F5E4D811501C</td>\n",
       "      <td>[-100]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, ...</td>\n",
       "      <td>[Limiting, car, usage, is, a, very, good, idea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  labels                                           ner_tags  \\\n",
       "0  C3811E7F1750  [-100]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  9B7494278BAE  [-100]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  93B9C33FF16D  [-100]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  ABE35EAFEC00  [-100]  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  F5E4D811501C  [-100]  [1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [BOOM!!, You're, on, I-75, on, the, ground, bl...  \n",
       "1  [Even, though, majority, of, humans, own, and,...  \n",
       "2  [Their, are, so, many, things, you, can, do, t...  \n",
       "3  [more, people, die, in, car, related, accidnts...  \n",
       "4  [Limiting, car, usage, is, a, very, good, idea...  "
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokenError.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n"
     ]
    }
   ],
   "source": [
    "# ner_tags < tokens by 1\n",
    "inputs = tokenizer(list(df_tokenError[\"tokens\"][0]), is_split_into_words=True)\n",
    "inputs.tokens()\n",
    "word_ids = inputs.word_ids()\n",
    "taggs = getIntArray(df_tokenError[\"ner_tags\"][0])\n",
    "print(len(df_tokenError[\"ner_tags\"][3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 0, 0, 0, 0, 1, 1, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 12, 13, 14, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 21, 22, 22, 23, 24, 25, 26, 27, 27, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 41, 42, 43, 44, 45, 46, 47, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 57, 58, 58, 59, 60, 61, 62, 62, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 76, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 86, 87, 88, 89, 90, 91, 92, 92, 93, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 132, 133, 134, 135, 136, 137, 137, 137, 138, 139, 140, 141, 142, 143, 144, 144, 145, 146, 146, 147, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 178, 179, 179, 179, 180, 181, 182, 183, 183, 184, 185, 185, 186, 187, 187, 187, 188, 188, 189, 190, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 226, 227, 228, 228, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 243, 244, 245, 245, 246, 247, 247, 248, 249, 250, 250, 250, 251, 251, 252, 253, 254, 254, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 279, 280, 281, 282, 283, 284, 285, 286, 287, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 308, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 321, None]\n"
     ]
    }
   ],
   "source": [
    "print(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-306-b0e6df535a60>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malign_labels_with_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtaggs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-139-2df8cc409b55>\u001b[0m in \u001b[0;36malign_labels_with_tokens\u001b[1;34m(labels, word_ids)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;31m# Start of a new word!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mcurrent_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword_id\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mnew_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mword_id\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "print(align_labels_with_tokens(taggs, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorRow = kaggleTrain.loc[kaggleTrain['id'] == \"C3811E7F1750\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BOOM!!',\n",
       " \"You're\",\n",
       " 'on',\n",
       " 'I-75',\n",
       " 'on',\n",
       " 'the',\n",
       " 'ground',\n",
       " 'bleeding',\n",
       " 'out',\n",
       " 'watching',\n",
       " 'everyone',\n",
       " 'surrounding',\n",
       " 'you,',\n",
       " 'calling',\n",
       " '911.',\n",
       " 'At',\n",
       " 'this',\n",
       " 'point',\n",
       " \"you're\",\n",
       " 'thinking',\n",
       " 'to',\n",
       " 'yourself,',\n",
       " '\"why',\n",
       " 'the',\n",
       " 'freak',\n",
       " 'did',\n",
       " 'this',\n",
       " 'happen?\"',\n",
       " 'A',\n",
       " 'couple',\n",
       " 'hours',\n",
       " 'later',\n",
       " 'you',\n",
       " 'found',\n",
       " 'out',\n",
       " 'an',\n",
       " 'adult',\n",
       " 'was',\n",
       " 'on',\n",
       " 'his',\n",
       " 'phone',\n",
       " 'texting',\n",
       " 'and',\n",
       " 'driving',\n",
       " 'before',\n",
       " 'the',\n",
       " 'wreck',\n",
       " 'happened,',\n",
       " 'which',\n",
       " 'is',\n",
       " 'the',\n",
       " 'most',\n",
       " 'selfish',\n",
       " 'thing',\n",
       " 'to',\n",
       " 'do',\n",
       " 'while',\n",
       " 'driving.',\n",
       " 'Putting',\n",
       " 'your',\n",
       " 'life',\n",
       " 'and',\n",
       " \"other's\",\n",
       " 'lives',\n",
       " 'in',\n",
       " 'danger',\n",
       " 'just',\n",
       " 'so',\n",
       " 'you',\n",
       " 'can',\n",
       " 'send',\n",
       " 'a',\n",
       " 'stupid',\n",
       " 'text',\n",
       " 'to',\n",
       " 'your',\n",
       " 'girl/boyfriend',\n",
       " 'is',\n",
       " 'completely',\n",
       " 'selfish',\n",
       " 'I',\n",
       " 'firmly',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'we',\n",
       " 'need',\n",
       " 'stricter',\n",
       " 'phone',\n",
       " 'laws',\n",
       " 'to',\n",
       " 'in',\n",
       " 'jail',\n",
       " 'people.',\n",
       " 'Texting',\n",
       " 'while',\n",
       " 'driving',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " 'top',\n",
       " 'five',\n",
       " 'causes',\n",
       " 'for',\n",
       " 'deaths,',\n",
       " 'and',\n",
       " 'yet',\n",
       " 'many',\n",
       " 'states',\n",
       " 'do',\n",
       " 'not',\n",
       " 'have',\n",
       " 'laws',\n",
       " 'against',\n",
       " 'it.',\n",
       " 'We',\n",
       " 'need',\n",
       " 'to',\n",
       " 'stand',\n",
       " 'together',\n",
       " 'united',\n",
       " 'and',\n",
       " 'encourage',\n",
       " 'our',\n",
       " 'Congressmen',\n",
       " 'to',\n",
       " 'pass',\n",
       " 'laws',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'combat',\n",
       " 'this',\n",
       " 'distraction.',\n",
       " 'and',\n",
       " 'be',\n",
       " 'mature',\n",
       " 'and',\n",
       " 'adult-like',\n",
       " 'and',\n",
       " 'discuss',\n",
       " 'ways',\n",
       " 'to',\n",
       " 'not',\n",
       " 'further',\n",
       " 'endanger',\n",
       " 'each',\n",
       " 'other.',\n",
       " 'Cellphones',\n",
       " 'are',\n",
       " 'brain',\n",
       " 'washing',\n",
       " 'us',\n",
       " 'to',\n",
       " 'think',\n",
       " 'we',\n",
       " 'always',\n",
       " 'need',\n",
       " 'it,',\n",
       " 'but',\n",
       " 'I',\n",
       " 'know',\n",
       " 'we',\n",
       " 'can',\n",
       " 'break',\n",
       " 'free',\n",
       " 'from',\n",
       " 'this',\n",
       " 'horrible',\n",
       " 'device',\n",
       " 'with',\n",
       " 'a',\n",
       " 'little',\n",
       " 'hard',\n",
       " 'work',\n",
       " 'we',\n",
       " 'can',\n",
       " 'make',\n",
       " 'a',\n",
       " 'change.',\n",
       " \"What's\",\n",
       " 'more',\n",
       " 'important',\n",
       " 'to',\n",
       " 'you:',\n",
       " 'a',\n",
       " 'notification',\n",
       " 'or',\n",
       " \"somebody's\",\n",
       " 'life?',\n",
       " 'Our',\n",
       " 'congressmen',\n",
       " 'need',\n",
       " 'to',\n",
       " 'listen',\n",
       " 'to',\n",
       " 'our',\n",
       " 'voices',\n",
       " 'and',\n",
       " 'make',\n",
       " 'bills',\n",
       " 'an',\n",
       " 'laws',\n",
       " 'and',\n",
       " 'reforms',\n",
       " 'to',\n",
       " 'make',\n",
       " 'the',\n",
       " 'roads',\n",
       " 'safer',\n",
       " 'for',\n",
       " 'everyone,',\n",
       " 'we',\n",
       " 'as',\n",
       " 'people',\n",
       " 'need',\n",
       " 'to',\n",
       " 'come',\n",
       " 'together',\n",
       " 'an',\n",
       " 'decide',\n",
       " 'what',\n",
       " 'to',\n",
       " 'do',\n",
       " 'to',\n",
       " 'make',\n",
       " 'things',\n",
       " 'happen,',\n",
       " 'we',\n",
       " \"don't\",\n",
       " 'know',\n",
       " 'how',\n",
       " 'much',\n",
       " 'more',\n",
       " 'accidents',\n",
       " 'have',\n",
       " 'to',\n",
       " 'happen',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'push',\n",
       " 'it',\n",
       " 'into',\n",
       " 'effect.',\n",
       " 'We',\n",
       " 'dedicate',\n",
       " 'our',\n",
       " 'selves',\n",
       " 'to',\n",
       " 'our',\n",
       " 'phones,we',\n",
       " 'cant',\n",
       " 'go',\n",
       " '10',\n",
       " 'minuets',\n",
       " 'without',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'see',\n",
       " 'if',\n",
       " 'we',\n",
       " 'got',\n",
       " 'a',\n",
       " 'notification',\n",
       " 'the',\n",
       " 'time',\n",
       " 'we',\n",
       " 'spend',\n",
       " 'on',\n",
       " 'our',\n",
       " 'phones',\n",
       " 'is',\n",
       " 'tremendous',\n",
       " 'its',\n",
       " 'up',\n",
       " 'to',\n",
       " '6',\n",
       " 'hours',\n",
       " 'a',\n",
       " 'day!',\n",
       " 'Not',\n",
       " 'including',\n",
       " 'the',\n",
       " 'time',\n",
       " 'spent',\n",
       " 'in',\n",
       " 'your',\n",
       " 'bed!',\n",
       " 'If',\n",
       " 'there',\n",
       " 'were',\n",
       " 'something',\n",
       " 'to',\n",
       " 'happen',\n",
       " 'to',\n",
       " 'all',\n",
       " 'of',\n",
       " 'our',\n",
       " 'phone',\n",
       " 'we',\n",
       " 'would',\n",
       " 'go',\n",
       " 'insane',\n",
       " 'if',\n",
       " 'we',\n",
       " 'would',\n",
       " 'just',\n",
       " 'make',\n",
       " 'stricker',\n",
       " 'laws',\n",
       " 'for',\n",
       " 'phone',\n",
       " 'an',\n",
       " 'driving',\n",
       " 'the',\n",
       " 'people',\n",
       " 'would',\n",
       " 'stop',\n",
       " 'because',\n",
       " 'of',\n",
       " 'the',\n",
       " 'consequences.']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "errorRow[\"tokens\"][12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n",
    "wro = [None, 0, 0, 0, 0, 0, 1, 1, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 12, 13, 14, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 21, 22, 22, 23, 24, 25, 26, 27, 27, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 41, 42, 43, 44, 45, 46, 47, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 57, 58, 58, 59, 60, 61, 62, 62, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 76, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 86, 87, 88, 89, 90, 91, 92, 92, 93, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 132, 133, 134, 135, 136, 137, 137, 137, 138, 139, 140, 141, 142, 143, 144, 144, 145, 146, 146, 147, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 178, 179, 179, 179, 180, 181, 182, 183, 183, 184, 185, 185, 186, 187, 187, 187, 188, 188, 189, 190, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 226, 227, 228, 228, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 243, 244, 245, 245, 246, 247, 247, 248, 249, 250, 250, 250, 251, 251, 252, 253, 254, 254, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 279, 280, 281, 282, 283, 284, 285, 286, 287, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 308, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 321, None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378\n",
      "380\n",
      "[-100, '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '1', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '3', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '2', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', '4', -100]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(kaggleTrain['ner_tags'][0], is_split_into_words=True)\n",
    "word_ids = inputs.word_ids()\n",
    "print(len(num_labels))\n",
    "print(len(word_ids))\n",
    "print(align_labels_with_tokens(kaggleTrain['ner_tags'][0], word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>C3811E7F1750</td>\n",
       "      <td>1.622473e+12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>461.0</td>\n",
       "      <td>BOOM!! You're on I-75 on the ground bleeding o...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>C3811E7F1750</td>\n",
       "      <td>1.622473e+12</td>\n",
       "      <td>462.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>I firmly believe that we need stricter phone l...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>80 81 82 83 84 85 86 87 88 89 90 91 92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>C3811E7F1750</td>\n",
       "      <td>1.622473e+12</td>\n",
       "      <td>532.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>Texting while driving is in the top five cause...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>93 94 95 96 97 98 99 100 101 102 103 104 105 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>C3811E7F1750</td>\n",
       "      <td>1.622473e+12</td>\n",
       "      <td>641.0</td>\n",
       "      <td>837.0</td>\n",
       "      <td>We need to stand together united and encourage...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>114 115 116 117 118 119 120 121 122 123 124 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>C3811E7F1750</td>\n",
       "      <td>1.622473e+12</td>\n",
       "      <td>838.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>Cellphones are brain washing us to think we al...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 2</td>\n",
       "      <td>147 148 149 150 151 152 153 154 155 156 157 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  discourse_id  discourse_start  discourse_end  \\\n",
       "99   C3811E7F1750  1.622473e+12              0.0          461.0   \n",
       "100  C3811E7F1750  1.622473e+12            462.0          531.0   \n",
       "101  C3811E7F1750  1.622473e+12            532.0          641.0   \n",
       "102  C3811E7F1750  1.622473e+12            641.0          837.0   \n",
       "103  C3811E7F1750  1.622473e+12            838.0          998.0   \n",
       "\n",
       "                                        discourse_text discourse_type  \\\n",
       "99   BOOM!! You're on I-75 on the ground bleeding o...           Lead   \n",
       "100  I firmly believe that we need stricter phone l...       Position   \n",
       "101  Texting while driving is in the top five cause...          Claim   \n",
       "102  We need to stand together united and encourage...       Evidence   \n",
       "103  Cellphones are brain washing us to think we al...          Claim   \n",
       "\n",
       "    discourse_type_num                                   predictionstring  \n",
       "99              Lead 1  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
       "100         Position 1             80 81 82 83 84 85 86 87 88 89 90 91 92  \n",
       "101            Claim 1  93 94 95 96 97 98 99 100 101 102 103 104 105 1...  \n",
       "102         Evidence 1  114 115 116 117 118 119 120 121 122 123 124 12...  \n",
       "103            Claim 2  147 148 149 150 151 152 153 154 155 156 157 15...  "
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_errorTest = pd.read_csv(\"train.csv\")\n",
    "singleErr = df_errorTest[df_errorTest[\"id\"] == \"C3811E7F1750\"]\n",
    "singleErr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>313.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>Some certain areas in the United States ban ph...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>402.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>When people have phones, they know about certa...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>759.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>Driving is one of the way how to get around. P...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "2  423A1CA112E2  1.622628e+12            313.0          401.0   \n",
       "3  423A1CA112E2  1.622628e+12            402.0          758.0   \n",
       "4  423A1CA112E2  1.622628e+12            759.0          886.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "1  They are some really bad consequences when stu...       Position   \n",
       "2  Some certain areas in the United States ban ph...       Evidence   \n",
       "3  When people have phones, they know about certa...       Evidence   \n",
       "4  Driving is one of the way how to get around. P...          Claim   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
       "1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n",
       "2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n",
       "3         Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n",
       "4            Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  "
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "7\n",
      "7\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "16\n",
      "16\n",
      "14\n",
      "14\n",
      "59\n",
      "59\n",
      "17\n",
      "17\n",
      "96\n",
      "96\n",
      "39\n",
      "40\n",
      "['Limiting', 'car', 'usage', 'is', 'a', 'very', 'good', 'idea', 'It', 'can', 'save', 'a', 'lot', 'of', 'money,', 'keep', 'pollution', 'from', 'happening', 'keep', 'people', 'safe.', 'If', 'more', 'people', 'tend', 'to', 'save', 'money', 'because', 'of', 'cars,', 'then', 'they', 'can', 'buy', 'more', 'stuff.', 'If', 'more', 'people', 'save', 'the', 'enviornment', 'people', 'can', 'have', 'fresh', 'air', 'to', 'breath', 'in.', '\"In', 'German', 'Suburb,', 'Life', 'Goes', 'On', 'Without', 'Cars\"', 'by', 'Elisabeth', 'Rosenthal,', 'it', 'states', 'that', '\"As', 'a', 'result', 'of', 'buying', 'a', 'parking', 'space', 'for', '$40,000,', '70', 'percent', 'of', \"Vauban's\", 'families', 'do', 'not', 'own', 'cars,', 'and', '57', 'percent', 'sold', 'a', 'car', 'to', 'move', 'here.', 'When', 'I', 'had', 'a', 'car', 'I', 'was', 'always', 'tense.', \"I'm\", 'much', 'happier', 'this', 'way,\"', 'said', 'Heidrun', 'Walter', 'This', 'quote', 'explains', 'the', 'troubles', 'and', 'tension', 'that', 'one', 'has', 'to', 'deal', 'with', 'when', 'having', 'a', 'car.', 'In', '\"The', 'End', 'of', 'Car', 'Culture\"', 'by', 'Elisabeth', 'Rosenthal,', 'the', 'author', 'supports', 'the', 'idea', 'of', 'limiting', 'car', 'usage', 'for', 'the', 'concern', 'of', 'safety.', 'Safety', 'is', 'a', 'really', 'important', 'thing', 'in', 'life.', 'Cars', 'can', 'limit', 'the', 'danger', 'of', 'getting', 'in', 'a', 'car', 'accident.', '\"He', 'proposed', 'partnering', 'with', 'the', 'telecommunications', 'industry', 'to', 'create', 'cities', 'in', 'which', 'pedistrians,', 'bicycle,', 'private', 'cars,', 'commercial', 'and', 'public', 'transportation', 'traffic', 'are', 'woven', 'into', 'a', 'connected', 'network', 'to', 'save', 'time,', 'conserve', 'resources,', 'lower', 'emissions,', 'and', 'improve', 'safety.\"', 'If', 'people', 'did', 'not', 'have', 'cars,', 'then', 'nobody', 'would', 'have', 'to', 'worry', 'about', 'accidents', 'and', 'their', 'safety.', ',', 'limiting', 'driving', 'and', 'the', 'usage', 'of', 'cars', 'can', 'improve', 'the', 'world.', 'It', 'can', 'make', 'it', 'a', 'safer', 'enviornment,', 'safe', 'money,', 'and', 'stop', 'pollution.', 'These', 'evidences', 'from', 'the', 'excerpts', 'can', 'prove', 'that', 'limiting', 'car', 'usage', 'is', 'a', 'really', 'good', 'idea.']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "df_file = train[train.id.str.contains(\"F5E4D811501C\")]\n",
    "df_file = df_file.reset_index()\n",
    "labels = []\n",
    "num_labels = []\n",
    "fullText = []\n",
    "for i in range(len(df_file.index)):\n",
    "    ps = df_file[\"predictionstring\"][i].split()\n",
    "    dt = df_file[\"discourse_text\"][i].split() \n",
    "    print(len(ps))\n",
    "    print(len(dt))\n",
    "    df_string = df_file[\"predictionstring\"][i]\n",
    "    stringArray = df_string.split()\n",
    "    df_textString = df_file[\"discourse_text\"][i].split()\n",
    "    for df in df_textString:\n",
    "        fullText.append(df)\n",
    "    for s in stringArray:\n",
    "        labels.append(df_file[\"discourse_type\"][i])\n",
    "        num_labels.append(tokenDict[str(df_file[\"discourse_type\"][i])])\n",
    "        \n",
    "\n",
    "print(fullText)\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378']\n"
     ]
    }
   ],
   "source": [
    "df_file = train[train.id.str.contains(\"423A1CA112E2\")]\n",
    "df_file = df_file.reset_index()\n",
    "idx = len(df_file) - 1\n",
    "ps = df_file[\"predictionstring\"][idx].split()\n",
    "dt = df_file[\"discourse_text\"][idx].split()\n",
    "print(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'news', 'always', 'updated', 'when', 'people', 'do', 'something', 'stupid', 'around', 'that', 'involves', 'their', 'phones.', 'The', 'safest', 'way', 'is', 'the', 'best', 'way', 'to', 'stay', 'safe.']\n"
     ]
    }
   ],
   "source": [
    "print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n"
     ]
    }
   ],
   "source": [
    "fileString = fileToArray(\"train/\" + str(train[\"id\"][0]) + \".txt\")\n",
    "fileArray = stringToArray(fileString)\n",
    "df_file = train[train.id.str.contains(str(train[\"id\"][0]),case=False)]\n",
    "print(len(fileString))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c931ba401747e1100110d99c7b2e1195adf3961a7e00160e720e39c4d164b397"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
