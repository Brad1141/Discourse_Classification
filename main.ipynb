{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "from datasets import Dataset\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (C:\\Users\\Brad\\.cache\\huggingface\\datasets\\conll2003\\conll2003\\1.0.0\\40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n",
      "100%|██████████| 3/3 [00:00<00:00, 264.34it/s]\n"
     ]
    }
   ],
   "source": [
    "raw_datasets  = load_dataset(\"conll2003\")\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "tokenDict = {\n",
    "    \"Lead\" : 0,\n",
    "    \"Position\" : 1,\n",
    "    \"Evidence\" : 2,\n",
    "    \"Claim\" : 3,\n",
    "    \"Concluding Statement\" : 4,\n",
    "    \"Counterclaim\" : 5,\n",
    "    \"Rebuttal\": 6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fileToArray(file_path):\n",
    "    # turn text into array of words\n",
    "    text_ds = tf.data.TextLineDataset(file_path).filter(lambda x: tf.cast(tf.strings.length(x), bool))\n",
    "    text_ds = text_ds.enumerate()\n",
    "    docTxt = []\n",
    "    for i in text_ds.as_numpy_iterator():\n",
    "        line = i[1].decode().split()\n",
    "        for l in line:\n",
    "            docTxt.append(l)\n",
    "    \n",
    "    docTxt = ' '.join(docTxt)\n",
    "    return docTxt\n",
    "\n",
    "def calc_word_indices(full_text, discourse_start, discourse_end):\n",
    "    start_index = len(full_text[:discourse_start].split())\n",
    "    token_len = len(full_text[discourse_start:discourse_end].split())\n",
    "    output = list(range(start_index, start_index + token_len))\n",
    "    if output[-1] >= len(full_text.split()):\n",
    "        output = list(range(start_index, start_index + token_len-1))\n",
    "    return output\n",
    "\n",
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            # Start of a new word!\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            # Special token\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            # Same word as previous token\n",
    "            label = labels[word_id]\n",
    "            # If the label is B-XXX we change it to I-XXX\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "\n",
    "    return new_labels\n",
    "\n",
    "def stringToArray(mystr):\n",
    "    wordList = re.sub(\"[^\\w]\", \" \",  mystr).split()\n",
    "    return wordList\n",
    "\n",
    "def getIntArray(strArrays):\n",
    "    intArrays = []\n",
    "    for strArray in strArrays:\n",
    "        intArray = []\n",
    "        for s in strArray:\n",
    "            intArray.append(int(s))\n",
    "        intArrays.append(intArray)\n",
    "    return intArrays\n",
    "\n",
    "def csv_to_df(fileName):\n",
    "    df = pd.read_csv(fileName)\n",
    "    df['tokens'] = df['tokens'].apply(lambda a: a.split())\n",
    "    df['ner_tags'] = df['ner_tags'].apply(lambda a: a.split())\n",
    "    df = df.drop(['Unnamed: 0'], axis = 1)\n",
    "    return df\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    all_labels = getIntArray(examples[\"ner_tags\"])\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        # try: \n",
    "        #     word_ids = tokenized_inputs.word_ids(i)\n",
    "        #     new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "        # except:\n",
    "        #     new_labels.append([-100])\n",
    "        \n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes in a csv file name and returns a tokenized dataset\n",
    "def createTokenDataset(fileName):\n",
    "    df = csv_to_df(fileName)\n",
    "    essayNames = df[\"id\"].unique()\n",
    "    \n",
    "    data = {\n",
    "        \"id\": [],\n",
    "        \"tokens\": [],\n",
    "        \"ner_tags\": []\n",
    "    }\n",
    "\n",
    "    token_df = pd.DataFrame(data)\n",
    "    for fileName in tqdm(essayNames):\n",
    "        df_file = df[df.id.str.contains(fileName,case=False)]\n",
    "        df_file = df_file.reset_index()\n",
    "        labels = []\n",
    "        num_labels = []\n",
    "        fullText = []\n",
    "        for i in range(len(df_file.index)):\n",
    "            df_string = df_file[\"predictionstring\"][i]\n",
    "            stringArray = df_string.split()\n",
    "            df_textString = df_file[\"discourse_text\"][i].split()\n",
    "            for df in df_textString:\n",
    "                if df != '.' and df != ',':\n",
    "                    fullText.append(df)\n",
    "            for s in stringArray:\n",
    "                labels.append(df_file[\"discourse_type\"][i])\n",
    "                num_labels.append(tokenDict[str(df_file[\"discourse_type\"][i])])\n",
    "\n",
    "        token_df.loc[len(token_df.index)] = [fileName, fullText, num_labels]\n",
    "    \n",
    "    #find a better way to do this\n",
    "    token_df['tokens'] = token_df['tokens'].apply(lambda a: ' '.join(map(str, a)))\n",
    "    token_df['ner_tags'] = token_df['ner_tags'].apply(lambda a: ' '.join(map(str, a)))\n",
    "    token_df['tokens'] = token_df['tokens'].apply(lambda a: a.split())\n",
    "    token_df['ner_tags'] = token_df['ner_tags'].apply(lambda a: a.split())\n",
    "\n",
    "    tokenDataset = Dataset.from_pandas(token_df)\n",
    "\n",
    "    tokenized_datasets = tokenDataset.map(\n",
    "        tokenize_and_align_labels,\n",
    "        batched=True,\n",
    "    )\n",
    "    return tokenized_datasets\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:04<?, ?ba/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not all arguments converted during string formatting",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-436-3631b582f518>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnewDataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m tokenized_datasets = newDataset.map(\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0mtokenize_and_align_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mbatched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2091\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2092\u001b[1;33m             return self._map_single(\n\u001b[0m\u001b[0;32m   2093\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2094\u001b[0m                 \u001b[0mwith_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Dataset\"\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"self\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[1;31m# apply actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DatasetDict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m         }\n\u001b[0;32m    484\u001b[0m         \u001b[1;31m# apply actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DatasetDict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[1;31m# re-apply format to the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[1;31m# Call actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[1;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   2467\u001b[0m                         )  # Something simpler?\n\u001b[0;32m   2468\u001b[0m                         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2469\u001b[1;33m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[0;32m   2470\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[1;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   2355\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2357\u001b[1;33m             \u001b[0mprocessed_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2358\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m                 \u001b[1;31m# Check if the function returns updated examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mdecorated\u001b[1;34m(item, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m                 )\n\u001b[0;32m   2051\u001b[0m                 \u001b[1;31m# Use the LazyDict internally, while mapping the function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2052\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecorated_item\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2053\u001b[0m                 \u001b[1;31m# Return a standard dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2054\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-435-e18412812fa1>\u001b[0m in \u001b[0;36mtokenize_and_align_labels\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mword_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[0mnew_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malign_labels_with_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[0mtokenized_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-435-e18412812fa1>\u001b[0m in \u001b[0;36malign_labels_with_tokens\u001b[1;34m(labels, word_ids)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;31m# If the label is B-XXX we change it to I-XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m                 \u001b[0mlabel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mnew_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: not all arguments converted during string formatting"
     ]
    }
   ],
   "source": [
    "train = csv_to_df(\"trainHugging5.csv\")\n",
    "newDataset = Dataset.from_pandas(train)\n",
    "\n",
    "tokenized_datasets = newDataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>C3811E7F1750</td>\n",
       "      <td>1.622473e+12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>461.0</td>\n",
       "      <td>BOOM!! You're on I-75 on the ground bleeding o...</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>C3811E7F1750</td>\n",
       "      <td>1.622473e+12</td>\n",
       "      <td>462.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>I firmly believe that we need stricter phone l...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>80 81 82 83 84 85 86 87 88 89 90 91 92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>C3811E7F1750</td>\n",
       "      <td>1.622473e+12</td>\n",
       "      <td>532.0</td>\n",
       "      <td>641.0</td>\n",
       "      <td>Texting while driving is in the top five cause...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>93 94 95 96 97 98 99 100 101 102 103 104 105 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>C3811E7F1750</td>\n",
       "      <td>1.622473e+12</td>\n",
       "      <td>641.0</td>\n",
       "      <td>837.0</td>\n",
       "      <td>We need to stand together united and encourage...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>114 115 116 117 118 119 120 121 122 123 124 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>C3811E7F1750</td>\n",
       "      <td>1.622473e+12</td>\n",
       "      <td>838.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>Cellphones are brain washing us to think we al...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 2</td>\n",
       "      <td>147 148 149 150 151 152 153 154 155 156 157 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  discourse_id  discourse_start  discourse_end  \\\n",
       "99   C3811E7F1750  1.622473e+12              0.0          461.0   \n",
       "100  C3811E7F1750  1.622473e+12            462.0          531.0   \n",
       "101  C3811E7F1750  1.622473e+12            532.0          641.0   \n",
       "102  C3811E7F1750  1.622473e+12            641.0          837.0   \n",
       "103  C3811E7F1750  1.622473e+12            838.0          998.0   \n",
       "\n",
       "                                        discourse_text discourse_type  \\\n",
       "99   BOOM!! You're on I-75 on the ground bleeding o...           Lead   \n",
       "100  I firmly believe that we need stricter phone l...       Position   \n",
       "101  Texting while driving is in the top five cause...          Claim   \n",
       "102  We need to stand together united and encourage...       Evidence   \n",
       "103  Cellphones are brain washing us to think we al...          Claim   \n",
       "\n",
       "    discourse_type_num                                   predictionstring  \n",
       "99              Lead 1  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
       "100         Position 1             80 81 82 83 84 85 86 87 88 89 90 91 92  \n",
       "101            Claim 1  93 94 95 96 97 98 99 100 101 102 103 104 105 1...  \n",
       "102         Evidence 1  114 115 116 117 118 119 120 121 122 123 124 12...  \n",
       "103            Claim 2  147 148 149 150 151 152 153 154 155 156 157 15...  "
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_errorTest = pd.read_csv(\"train.csv\")\n",
    "singleErr = df_errorTest[df_errorTest[\"id\"] == \"C3811E7F1750\"]\n",
    "singleErr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>discourse_id</th>\n",
       "      <th>discourse_start</th>\n",
       "      <th>discourse_end</th>\n",
       "      <th>discourse_text</th>\n",
       "      <th>discourse_type</th>\n",
       "      <th>discourse_type_num</th>\n",
       "      <th>predictionstring</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>8.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>Modern humans today are always on their phone....</td>\n",
       "      <td>Lead</td>\n",
       "      <td>Lead 1</td>\n",
       "      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>230.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>They are some really bad consequences when stu...</td>\n",
       "      <td>Position</td>\n",
       "      <td>Position 1</td>\n",
       "      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>313.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>Some certain areas in the United States ban ph...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 1</td>\n",
       "      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>402.0</td>\n",
       "      <td>758.0</td>\n",
       "      <td>When people have phones, they know about certa...</td>\n",
       "      <td>Evidence</td>\n",
       "      <td>Evidence 2</td>\n",
       "      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>1.622628e+12</td>\n",
       "      <td>759.0</td>\n",
       "      <td>886.0</td>\n",
       "      <td>Driving is one of the way how to get around. P...</td>\n",
       "      <td>Claim</td>\n",
       "      <td>Claim 1</td>\n",
       "      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id  discourse_id  discourse_start  discourse_end  \\\n",
       "0  423A1CA112E2  1.622628e+12              8.0          229.0   \n",
       "1  423A1CA112E2  1.622628e+12            230.0          312.0   \n",
       "2  423A1CA112E2  1.622628e+12            313.0          401.0   \n",
       "3  423A1CA112E2  1.622628e+12            402.0          758.0   \n",
       "4  423A1CA112E2  1.622628e+12            759.0          886.0   \n",
       "\n",
       "                                      discourse_text discourse_type  \\\n",
       "0  Modern humans today are always on their phone....           Lead   \n",
       "1  They are some really bad consequences when stu...       Position   \n",
       "2  Some certain areas in the United States ban ph...       Evidence   \n",
       "3  When people have phones, they know about certa...       Evidence   \n",
       "4  Driving is one of the way how to get around. P...          Claim   \n",
       "\n",
       "  discourse_type_num                                   predictionstring  \n",
       "0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n",
       "1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n",
       "2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n",
       "3         Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n",
       "4            Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  "
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "8\n",
      "7\n",
      "7\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "16\n",
      "16\n",
      "14\n",
      "14\n",
      "59\n",
      "59\n",
      "17\n",
      "17\n",
      "96\n",
      "96\n",
      "39\n",
      "40\n",
      "['Limiting', 'car', 'usage', 'is', 'a', 'very', 'good', 'idea', 'It', 'can', 'save', 'a', 'lot', 'of', 'money,', 'keep', 'pollution', 'from', 'happening', 'keep', 'people', 'safe.', 'If', 'more', 'people', 'tend', 'to', 'save', 'money', 'because', 'of', 'cars,', 'then', 'they', 'can', 'buy', 'more', 'stuff.', 'If', 'more', 'people', 'save', 'the', 'enviornment', 'people', 'can', 'have', 'fresh', 'air', 'to', 'breath', 'in.', '\"In', 'German', 'Suburb,', 'Life', 'Goes', 'On', 'Without', 'Cars\"', 'by', 'Elisabeth', 'Rosenthal,', 'it', 'states', 'that', '\"As', 'a', 'result', 'of', 'buying', 'a', 'parking', 'space', 'for', '$40,000,', '70', 'percent', 'of', \"Vauban's\", 'families', 'do', 'not', 'own', 'cars,', 'and', '57', 'percent', 'sold', 'a', 'car', 'to', 'move', 'here.', 'When', 'I', 'had', 'a', 'car', 'I', 'was', 'always', 'tense.', \"I'm\", 'much', 'happier', 'this', 'way,\"', 'said', 'Heidrun', 'Walter', 'This', 'quote', 'explains', 'the', 'troubles', 'and', 'tension', 'that', 'one', 'has', 'to', 'deal', 'with', 'when', 'having', 'a', 'car.', 'In', '\"The', 'End', 'of', 'Car', 'Culture\"', 'by', 'Elisabeth', 'Rosenthal,', 'the', 'author', 'supports', 'the', 'idea', 'of', 'limiting', 'car', 'usage', 'for', 'the', 'concern', 'of', 'safety.', 'Safety', 'is', 'a', 'really', 'important', 'thing', 'in', 'life.', 'Cars', 'can', 'limit', 'the', 'danger', 'of', 'getting', 'in', 'a', 'car', 'accident.', '\"He', 'proposed', 'partnering', 'with', 'the', 'telecommunications', 'industry', 'to', 'create', 'cities', 'in', 'which', 'pedistrians,', 'bicycle,', 'private', 'cars,', 'commercial', 'and', 'public', 'transportation', 'traffic', 'are', 'woven', 'into', 'a', 'connected', 'network', 'to', 'save', 'time,', 'conserve', 'resources,', 'lower', 'emissions,', 'and', 'improve', 'safety.\"', 'If', 'people', 'did', 'not', 'have', 'cars,', 'then', 'nobody', 'would', 'have', 'to', 'worry', 'about', 'accidents', 'and', 'their', 'safety.', ',', 'limiting', 'driving', 'and', 'the', 'usage', 'of', 'cars', 'can', 'improve', 'the', 'world.', 'It', 'can', 'make', 'it', 'a', 'safer', 'enviornment,', 'safe', 'money,', 'and', 'stop', 'pollution.', 'These', 'evidences', 'from', 'the', 'excerpts', 'can', 'prove', 'that', 'limiting', 'car', 'usage', 'is', 'a', 'really', 'good', 'idea.']\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4]\n"
     ]
    }
   ],
   "source": [
    "df_file = train[train.id.str.contains(\"F5E4D811501C\")]\n",
    "df_file = df_file.reset_index()\n",
    "labels = []\n",
    "num_labels = []\n",
    "fullText = []\n",
    "for i in range(len(df_file.index)):\n",
    "    ps = df_file[\"predictionstring\"][i].split()\n",
    "    dt = df_file[\"discourse_text\"][i].split() \n",
    "    print(len(ps))\n",
    "    print(len(dt))\n",
    "    df_string = df_file[\"predictionstring\"][i]\n",
    "    stringArray = df_string.split()\n",
    "    df_textString = df_file[\"discourse_text\"][i].split()\n",
    "    for df in df_textString:\n",
    "        fullText.append(df)\n",
    "    for s in stringArray:\n",
    "        labels.append(df_file[\"discourse_type\"][i])\n",
    "        num_labels.append(tokenDict[str(df_file[\"discourse_type\"][i])])\n",
    "        \n",
    "\n",
    "print(fullText)\n",
    "print(num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['355', '356', '357', '358', '359', '360', '361', '362', '363', '364', '365', '366', '367', '368', '369', '370', '371', '372', '373', '374', '375', '376', '377', '378']\n"
     ]
    }
   ],
   "source": [
    "df_file = train[train.id.str.contains(\"423A1CA112E2\")]\n",
    "df_file = df_file.reset_index()\n",
    "idx = len(df_file) - 1\n",
    "ps = df_file[\"predictionstring\"][idx].split()\n",
    "dt = df_file[\"discourse_text\"][idx].split()\n",
    "print(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'news', 'always', 'updated', 'when', 'people', 'do', 'something', 'stupid', 'around', 'that', 'involves', 'their', 'phones.', 'The', 'safest', 'way', 'is', 'the', 'best', 'way', 'to', 'stay', 'safe.']\n"
     ]
    }
   ],
   "source": [
    "print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>[Modern, humans, today, are, always, on, their...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A8445CABFECE</td>\n",
       "      <td>[Drivers, should, not, be, able, to, use, phon...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6B4F7A0165B9</td>\n",
       "      <td>[The, ability, to, stay, connected, to, people...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>[People, are, debating, whether, if, drivers, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50B3435E475B</td>\n",
       "      <td>[Over, half, of, drivers, in, today's, society...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                             tokens  \\\n",
       "0  423A1CA112E2  [Modern, humans, today, are, always, on, their...   \n",
       "1  A8445CABFECE  [Drivers, should, not, be, able, to, use, phon...   \n",
       "2  6B4F7A0165B9  [The, ability, to, stay, connected, to, people...   \n",
       "3  E05C7F5C1156  [People, are, debating, whether, if, drivers, ...   \n",
       "4  50B3435E475B  [Over, half, of, drivers, in, today's, society...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixTrain = csv_to_df(\"trainHugging4.csv\")\n",
    "fixTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1070\n"
     ]
    }
   ],
   "source": [
    "fixTrainSample = fixTrain[fixTrain[\"id\"] == 'F5E4D811501C']\n",
    "print(list(fixTrainSample.index)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "longTokens = list(fixTrainSample[\"tokens\"])[0]\n",
    "for t in longTokens:\n",
    "    if t == '.' or t == ',':\n",
    "        print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorIds = list(df_tokenError[\"id\"])\n",
    "for e in errorIds:\n",
    "    row = fixTrain[fixTrain[\"id\"] == e]\n",
    "    tokens = list(row[\"tokens\"])[0]\n",
    "    for t in tokens:\n",
    "        if t == '.' or t == ',':\n",
    "            print(\"r\")\n",
    "            tokens.remove(t)\n",
    "    fixTrain.at[list(row.index)[0], \"tokens\"] = tokens.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"id\": errorIds\n",
    "}\n",
    "\n",
    "token_df = pd.DataFrame(data)\n",
    "token_df.to_csv(\"Error_Ids.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags'],\n",
       "    num_rows: 15594\n",
       "})"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixData = Dataset.from_pandas(fixTrain)\n",
    "fixData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "378\n",
      "334\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "not all arguments converted during string formatting",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-509-74e81e16c9a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malign_labels_with_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-500-25e3b6981659>\u001b[0m in \u001b[0;36malign_labels_with_tokens\u001b[1;34m(labels, word_ids)\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[1;31m# If the label is B-XXX we change it to I-XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m                 \u001b[0mlabel\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mnew_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: not all arguments converted during string formatting"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(fixData[1070][\"tokens\"], is_split_into_words=True)\n",
    "\n",
    "labels = fixData[0][\"ner_tags\"]\n",
    "word_ids = inputs.word_ids()\n",
    "print(len(labels))\n",
    "print(len(word_ids))\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixTrain.to_csv(\"trainHugging5.csv\")\n",
    "fixTrain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [],
   "source": [
    "newTrain = csv_to_df(\"trainHugging4.csv\")\n",
    "# newTrain = newTrain.drop(['Unnamed: 0'], axis = 1)\n",
    "# for index, row in newTrain.iterrows():\n",
    "#     tags = []\n",
    "#     for c in row[2]:\n",
    "#         try:\n",
    "#             tags.append(int(c))\n",
    "#         except:\n",
    "#             continue\n",
    "#     newTrain.at[index, \"ner_tags\"] = tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>423A1CA112E2</td>\n",
       "      <td>[Modern, humans, today, are, always, on, their...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A8445CABFECE</td>\n",
       "      <td>[Drivers, should, not, be, able, to, use, phon...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6B4F7A0165B9</td>\n",
       "      <td>[The, ability, to, stay, connected, to, people...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E05C7F5C1156</td>\n",
       "      <td>[People, are, debating, whether, if, drivers, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50B3435E475B</td>\n",
       "      <td>[Over, half, of, drivers, in, today's, society...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                             tokens  \\\n",
       "0  423A1CA112E2  [Modern, humans, today, are, always, on, their...   \n",
       "1  A8445CABFECE  [Drivers, should, not, be, able, to, use, phon...   \n",
       "2  6B4F7A0165B9  [The, ability, to, stay, connected, to, people...   \n",
       "3  E05C7F5C1156  [People, are, debating, whether, if, drivers, ...   \n",
       "4  50B3435E475B  [Over, half, of, drivers, in, today's, society...   \n",
       "\n",
       "                                            ner_tags  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 485,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newTrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "newDataset = Dataset.from_pandas(newTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:03<?, ?ba/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-501-4be759a97cbc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m tokenized_datasets = fixData.map(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mtokenize_and_align_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mbatched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2091\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mnum_proc\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2092\u001b[1;33m             return self._map_single(\n\u001b[0m\u001b[0;32m   2093\u001b[0m                 \u001b[0mfunction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2094\u001b[0m                 \u001b[0mwith_indices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwith_indices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Dataset\"\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"self\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[1;31m# apply actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DatasetDict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    483\u001b[0m         }\n\u001b[0;32m    484\u001b[0m         \u001b[1;31m# apply actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DatasetDict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[1;31m# re-apply format to the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    409\u001b[0m             \u001b[1;31m# Call actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 411\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    412\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    413\u001b[0m             \u001b[1;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   2467\u001b[0m                         )  # Something simpler?\n\u001b[0;32m   2468\u001b[0m                         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2469\u001b[1;33m                             batch = apply_function_on_filtered_inputs(\n\u001b[0m\u001b[0;32m   2470\u001b[0m                                 \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2471\u001b[0m                                 \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mapply_function_on_filtered_inputs\u001b[1;34m(inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[0;32m   2355\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mwith_rank\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2356\u001b[0m                 \u001b[0madditional_args\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2357\u001b[1;33m             \u001b[0mprocessed_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0madditional_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2358\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2359\u001b[0m                 \u001b[1;31m# Check if the function returns updated examples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mdecorated\u001b[1;34m(item, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2050\u001b[0m                 )\n\u001b[0;32m   2051\u001b[0m                 \u001b[1;31m# Use the LazyDict internally, while mapping the function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2052\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecorated_item\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2053\u001b[0m                 \u001b[1;31m# Return a standard dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2054\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLazyDict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-500-25e3b6981659>\u001b[0m in \u001b[0;36mtokenize_and_align_labels\u001b[1;34m(examples)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mword_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenized_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_ids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mnew_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malign_labels_with_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[0mtokenized_inputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"labels\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-500-25e3b6981659>\u001b[0m in \u001b[0;36malign_labels_with_tokens\u001b[1;34m(labels, word_ids)\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;31m# Start of a new word!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m             \u001b[0mcurrent_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m             \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m100\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword_id\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m             \u001b[0mnew_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mword_id\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = fixData.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n"
     ]
    }
   ],
   "source": [
    "fileString = fileToArray(\"train/\" + str(train[\"id\"][0]) + \".txt\")\n",
    "fileArray = stringToArray(fileString)\n",
    "df_file = train[train.id.str.contains(str(train[\"id\"][0]),case=False)]\n",
    "print(len(fileString))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c931ba401747e1100110d99c7b2e1195adf3961a7e00160e720e39c4d164b397"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
